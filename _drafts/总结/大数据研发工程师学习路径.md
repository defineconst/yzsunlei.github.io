## 走进大数据平台研发
* 大数据特征：规模大、信息全、维度细、时效强
* 数据平台能做的：提供**灵活、易用**的大数据处理能力；为上层**数据应用**提供基础支持
* 与BI工程师的区别和联系：各类业务指标计算
* 与数据分析师的区别和联系：根据业务指标得出分析结论
* 与推荐算法工程师的区别和联系：拿到算法所需的数据并进行预处理，使用机器学习算法对数据进行训练并输出训练模型
* 与分布式存储系统工程师的区别和联系：研发高效的分布式存储系统
* 促进需求发展的几大要素：能采集到需要的数据；拥有足够的数据处理能力；对数据应用有明确且充分的需求
* 市场对大数据工程师的职责：负责公司大数据离线、实时平台(Hadoop/Hive/Storm/Spark)的建设、优化；负责开发大数据工具，如报表平台、多维度分析工具、ETL平台、调度平台的研发
* 市场对大数据工程师的岗位要求：熟悉Hadoop、Hive、Spark、Storm的原理、优化；熟悉数据仓库理论；熟悉开源大数据平台如HBase、ES、Kylin、Druid等；熟练进行Java、Python的代码编写和良好的算法技能
* 数据平台要解决的问题：为什么需要数据(业务需求)；需要哪些数据(数据源)；数据从哪来(数据采集、传输)；数据放在哪里(数据存储)；数据怎么处理(数据计算、查询)
* 数据采集：不同的数据类型(用户行为数据、运维日志数据、语音图像数据)；不同的采集手段(客户端采集、服务端采集)；不同的数据源(日志文件、数据库、业务系统)
* 数据传输：传输方式(实时/批量、增量/全量)；传输目的地(文件系统、数据库)；容错要求(严格一致、允许丢失、允许重复)
* 数据存储：逻辑模型(表格、Key-Value、非结构化)；访问模式(批量访问、随机访问)；更新模式(只追加不更新、低频更新、高频随机更新)；可靠性(多副本、内存存储)
* 数据计算：计算模型(MapReduce、DAG)；计算方式(批处理、实时)；编程接口(Java、Python)
* 数据查询：数据在哪(数据库、文件系统)；查询目的(简单统计、组合分析、数据抽取、格式处理)；查询频率(高频、低频)
* 学习路径：职业入门(合格的程序员，了解业务需求)-初级工程师(解决实际的业务问题，了解相关概念和理论)-中级工程师(充分了解数据平台相关特性，深入了解平台组件原理)-高级工程师(深入理解业务，对需求进行抽象和整合)

## 数据平台概念与技术
* 大数据面临的挑战：数据爆炸式增长；数据类型多样化、数据类型多；数据批量处理；数据价值密度低
* 数据平台具备的能力：数据采集-数据存储-离线数据分析-实时数据分析-数据可视化
* 数据采集，数据平台最基本的功能(离线数据、实时数据)
* 数据存储(结构化数据、非结构化数据)
* 离线数据分析：基本的分析能力(计算和存储分离)，整合多数据源(计算引擎要支持多数据源)，提升查询速度(持续优化)
* 实时数据分析：地位(实时为主，离线为辅)，实时数据分析的能力(实时、稳定、统一逻辑)
* 数据可视化：门面(一定要重视)
* 数据平台：集数据收集、存储、分析、可视化于一体的，面向整个公司的产品

* 数据平台技术：数据采集技术、数据存储技术、离线数据分析技术、实时数据分析技术、数据可视化技术
* 离线采集数据库数据：sqoop、datax
* 实时采集数据库数据：canel(免费，阿里开源)、Oracle GoldenGate(收费)
* 日志采集数据：Flume、Logstash、Scribe
* 分布式文件系统：hdfs(传统的)、alluxio(内存式)
* 键值型数据库：hbase(基于磁盘)、redis(基于内存，现存最大的存储系统)
* 分布式消息队列：kafka（更好的性能就采用它）、rocketmq
* 离线数据分析：Mapreduce、Spark、impala
* 实时数据分析：storm、spark streaming(纯实时计算、实时性高、和hadoop生态圈结合紧密)；两者关系：相辅相成、针对不同业务
* 数据化可视化特点：个性化
* 数据可视化技术：hue、zeppelin

* 数据统一管理：资源有效利用、打通数据关系、统一数据逻辑
* 统一数据服务：统一数据出口、降低用户使用难度、用户行为监控和权限管理
* 数据安全：老生常谈的话题、基本职责之一、数据备份、数据恢复、数据加密、数据权限控制
* 数据平台与数据仓库的关系：提供自动化脚本、维护任务调度系统
* 数据平台和分析类部门的关系：提供高可用的数据、提供高效的计算引擎、提供友好的分析工具
* 数据平台与数据挖掘的关系：提供高质量的数据、提供高效和易用的计算引擎
* 数据平台和业务系统的关系：提供数据支持、提供技术框架、预警

## 如何进行数据ETL
* ETL基本概念：Extract-Transform-Load的缩写；商务智能与数据仓库的核心；统一规则集成并提高数据价值；完成数据从数据源到目标数据仓库转换的过程
* 包含的内容：抽取(Extract)-转换(Transform)-装载(Load)
* 准备区包含的内容：异构访问、数据增量迭代、简化开发
* 需要使用时间戳的原因：标识维度信息、增量抽取
* 日志表的运用：进度监控、任务记录(Finished，Failed，Spent等)
* 使用调度器的原因：增量更新、定期运行、任务告警
* 游戏日志数据过滤的类型：不完整数据、错误数据、重复数据
* 数据转化流程：选择-分离/合并-转化-汇总和补齐
* 数据转化的任务类型：数据格式转换、数据粒度转换、规则计算
* 数据导入方法：利用工具导入数据(通用性，扩展性，可维护性)；利用API接口导入(自主开发，可控性强)
* 数据来源分析包含的内容：离线日志(压缩文件)、完整性(MD5)、合法性(KPI)
* 进行数据预处理所涉及的内容点：错误数据矫正、数据去重处理、脏数据过滤
* 数据分流导向：数据仓库(Hive)、分布式存储数据(HDFS)、海量数据分析存储(HBase)、即时数据搜索(Elasticsearch)
* Hive SQL：适合处理离线任务；易使用，学习成本帝；通用性，可移植性
* MapReduce：适合处理离线任务(吞吐，时延，静态数据)；学习成本高(需要有编程语言基础)；通用性弱
* Spark：批处理和流数据两者兼容；学习成本高(编程语言基础)；拥有SQL的功能
* Hive SQL实现：统计SQL编写；任务运行；代码演示
* MapReduce实现：IDE准备及工程创建；添加Hadoop依赖包；编写MR代码；代码演示
* Spark实现：IDE准备及工程创建(Java或者Scala)；添加Hadoop，Spark依赖包；编写Spark代码；代码演示

## 数据可视化之Echarts
* 怎么理解数据可视化：不只是数字(数据是现实生活的一种映射，讲故事的方式完全取决于你自己)；我们寻求什么(模式、相互关系、问题)；设计(解释编码、标注、确保准确性、提供数据来源、考虑受众)
* 数据可视化开发库：R、Highcharts、Echarts、D3、Polymaps、Processing、Protovis、Google Chart Tools
* 数据可视化工具：IBM Many Eyes、Tableau、Google Fusion Tables
* 数据可视化服务：DataV(阿里)、RayData(腾讯)

* 第一个Echarts图表：开发环境准备、下载并引入Echarts、下载并引入主题、初始化Echarts、设置图表的配置项和数据、显示图表
* 基本图：折线(面积)图、柱状(条形)图、饼图、雷达图、仪表盘、漏斗图、矩形树图、字符云、多图混搭
* 高级图：散点图、地图、线图、K线图、箱线图、热力图、关系图、桑基图、水球图
* 多图联动(时间轴)：timeline、autoPlay、playInterval

## 数据采集方案
* 数据采集是什么：为了满足统计、分析、挖掘的需要，搜集和获取各种数据的过程，数据驱动、精细化运营的根本
* 数据采集目标：数据类型（用户行为数据、用户数据、业务运行数据、内容数据等）、所有者（第一方数据、第三方数据）
* 一个典型的互联网用户产品：前端操作（JavaScript、IOS、Android，按钮点击、下拉框选择），后端日志（Nginx、UI、Server，浏览、检索、购买、支付），业务操作（数据库、CRM，物流、进货、客服）
* 数据采集的应用场景：个性化统计、在线分析、搜索优化、用户画像、文本挖掘、反作弊、精准广告

* 代码埋点的基本原理：在事件/行为发生时，显示的调用代码发送记录
* 后端会采集哪些数据：后端日志，后端服务响应情况的纪录，对用户操作的最终体现；第三方数据，对接的第三方服务提供的数据，为用户提供的服务的体现
* 后端数据采集的原理：对数据按照要求进行处理(数据产生前/产生后，格式处理/内容处理)；将处理后的数据用相应接口发送给数据处理/分析平台
* 后端日志采集面临的挑战：文本日志的解析与解读；日志格式变更的向前向后兼容；日志的收集与传输；软件开发过程中的日志变更管理
* 后端数据库采集面临的挑战：对生产环境的影响；时效性无法满足；缺乏历史状态
* 后端第三方数据采集面临的挑战：API不通用；采集能力受限
* 一个较好的日志内容实践-用户行为日志五要素：Who(设备ID、注册ID等)、When(事件发生时间等)、Where(IP和GPS信息以及由此解析出来的地域、LBS信息等)、How(浏览器、操作系统、设备型号、Referer等)、What(URL、Title、表单内容等)
* 可用的日志采集与传输方案：打印到本地文件，然后批量下载/传输；打印到本地文件，然后用agent实时tail与传输；通过网络直接打印到日志中央服务器
* 代码埋点的优点：控制精准，可以设置自定义属性；缺点：埋点代价大，发布代价大

* 数据采集考虑的问题：便捷性(部署方便)、采集能力(采集所需)、准确性(避免数据损失)、安全与隐私(造成数据泄露和伪造)

## 实现一个实时数据统计系统
* Netty：非常流行的Java Nio网络通信框架，能够接收TCP、Http、Websocket、Https、UDP等协议发来的通信消息netty高并发服务器
* ELK包含：LogStash(tcp监听netty服务器发送的消息，并把消息处理好转发给ES)、Elasticsearch(接受LogStash发送来的数据，并进行分词构建索引)、Kibana(主要是从ES中获取相关索引的数据进行检索，分析)

## PB级分布式查询引擎应用解析


## 机器学习基石篇



















